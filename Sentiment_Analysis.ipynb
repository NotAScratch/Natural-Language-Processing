{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6x6ZdJDUJFkx",
        "outputId": "270447eb-f722-4a48-9cff-fdab7cfdf234"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/workspaces/Natural-Language-Processing/Sentiment_Analysis.ipynb Cell 1\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bzany-fishstick-v4wg9wqgr56fpv6r/workspaces/Natural-Language-Processing/Sentiment_Analysis.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bzany-fishstick-v4wg9wqgr56fpv6r/workspaces/Natural-Language-Processing/Sentiment_Analysis.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score, classification_report, confusion_matrix\n\u001b[0;32m----> <a href='vscode-notebook-cell://codespaces%2Bzany-fishstick-v4wg9wqgr56fpv6r/workspaces/Natural-Language-Processing/Sentiment_Analysis.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec\n\u001b[1;32m      <a href='vscode-notebook-cell://codespaces%2Bzany-fishstick-v4wg9wqgr56fpv6r/workspaces/Natural-Language-Processing/Sentiment_Analysis.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m     <a href='vscode-notebook-cell://codespaces%2Bzany-fishstick-v4wg9wqgr56fpv6r/workspaces/Natural-Language-Processing/Sentiment_Analysis.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Function to preprocess and vectorize data\u001b[39;00m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/gensim/__init__.py:11\u001b[0m\n\u001b[1;32m      7\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m4.3.3\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[1;32m     14\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m'\u001b[39m\u001b[39mgensim\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m logger\u001b[39m.\u001b[39mhandlers:  \u001b[39m# To ensure reload() doesn't add another one\u001b[39;00m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/gensim/corpora/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mindexedcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m IndexedCorpus  \u001b[39m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmmcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m MmCorpus  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbleicorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m BleiCorpus  \u001b[39m# noqa:F401\u001b[39;00m\n",
            "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/gensim/corpora/indexedcorpus.py:14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m interfaces, utils\n\u001b[1;32m     16\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mIndexedCorpus\u001b[39;00m(interfaces\u001b[39m.\u001b[39mCorpusABC):\n",
            "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/gensim/interfaces.py:19\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[39mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m utils, matutils\n\u001b[1;32m     22\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCorpusABC\u001b[39;00m(utils\u001b[39m.\u001b[39mSaveLoad):\n",
            "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/gensim/matutils.py:1034\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1.\u001b[39m \u001b[39m-\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mlen\u001b[39m(set1 \u001b[39m&\u001b[39m set2)) \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(union_cardinality)\n\u001b[1;32m   1032\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1033\u001b[0m     \u001b[39m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[0;32m-> 1034\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_matutils\u001b[39;00m \u001b[39mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[1;32m   1036\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m   1037\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mlogsumexp\u001b[39m(x):\n",
            "File \u001b[0;32m~/.python/current/lib/python3.12/site-packages/gensim/_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# Importing Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Function to preprocess and vectorize data\n",
        "def preprocess_and_vectorize(X_train, X_test, vectorizer):\n",
        "    X_train_vec = vectorizer.fit_transform(X_train)\n",
        "    X_test_vec = vectorizer.transform(X_test)\n",
        "    return X_train_vec, X_test_vec\n",
        "\n",
        "# Function to get Word2Vec features\n",
        "def get_w2v_features(tokens, model, vector_size):\n",
        "    features = []\n",
        "    for sentence in tokens:\n",
        "        vec = np.zeros(vector_size)\n",
        "        count = 0\n",
        "        for word in sentence:\n",
        "            if word in model.wv.key_to_index:\n",
        "                vec += model.wv[word]\n",
        "                count += 1\n",
        "        if count > 0:\n",
        "            vec /= count\n",
        "        features.append(vec)\n",
        "    return np.array(features)\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, X_test_vec, y_test_enc):\n",
        "    y_pred = model.predict(X_test_vec)\n",
        "    accuracy = accuracy_score(y_test_enc, y_pred)\n",
        "    print(f\"Accuracy: {accuracy:.2f}\\n\")\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test_enc, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_enc, y_pred))\n",
        "    print(\"-\" * 50)\n",
        "    return accuracy\n",
        "\n",
        "# Step 1: Create Dummy Data\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I love this product, it's amazing!\",\n",
        "        \"This is the worst experience I've ever had.\",\n",
        "        \"Absolutely fantastic! Highly recommend.\",\n",
        "        \"Not great, would not buy again.\",\n",
        "        \"The quality is superb, really satisfied.\",\n",
        "        \"Terrible, broke after one use.\",\n",
        "        \"Decent product for the price.\",\n",
        "        \"Awful customer service, very disappointed.\",\n",
        "        \"Excellent value for money, very happy.\",\n",
        "        \"It's okay, nothing special but works fine.\"\n",
        "    ],\n",
        "    \"sentiment\": [\"positive\", \"negative\", \"positive\", \"negative\", \"positive\",\n",
        "                  \"negative\", \"neutral\", \"negative\", \"positive\", \"neutral\"]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 2: Preprocessing\n",
        "X = df['text']\n",
        "y = df['sentiment']\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Encoding labels for consistency\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_enc = label_encoder.fit_transform(y_train)\n",
        "y_test_enc = label_encoder.transform(y_test)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 2))  # Using unigrams and bigrams\n",
        "X_train_tfidf, X_test_tfidf = preprocess_and_vectorize(X_train, X_test, tfidf_vectorizer)\n",
        "\n",
        "# Bag of Words (BoW) Vectorization\n",
        "bow_vectorizer = CountVectorizer(max_features=500, ngram_range=(1, 2))\n",
        "X_train_bow, X_test_bow = preprocess_and_vectorize(X_train, X_test, bow_vectorizer)\n",
        "\n",
        "# Word2Vec Vectorization\n",
        "X_train_tokens = [sentence.split() for sentence in X_train]\n",
        "X_test_tokens = [sentence.split() for sentence in X_test]\n",
        "\n",
        "w2v_model = Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
        "X_train_w2v = get_w2v_features(X_train_tokens, w2v_model, vector_size=100)\n",
        "X_test_w2v = get_w2v_features(X_test_tokens, w2v_model, vector_size=100)\n",
        "\n",
        "# Step 3: Model Training and Evaluation\n",
        "methods = {\n",
        "    \"TF-IDF\": (X_train_tfidf, X_test_tfidf),\n",
        "    \"Bag of Words\": (X_train_bow, X_test_bow),\n",
        "    \"Word2Vec\": (X_train_w2v, X_test_w2v)\n",
        "}\n",
        "\n",
        "models = {}\n",
        "results = {}\n",
        "\n",
        "for method, (X_train_vec, X_test_vec) in methods.items():\n",
        "    print(f\"Evaluating with {method}...\")\n",
        "    model = LogisticRegression(max_iter=200)\n",
        "    model.fit(X_train_vec, y_train_enc)\n",
        "    accuracy = evaluate_model(model, X_test_vec, y_test_enc)\n",
        "    results[method] = accuracy\n",
        "    models[method] = model\n",
        "\n",
        "# Compare Results\n",
        "print(\"\\nComparison of Accuracy:\")\n",
        "for method, accuracy in results.items():\n",
        "    print(f\"{method}: {accuracy:.2f}\")\n",
        "\n",
        "# Step 4: Predict Sentiment for User Input\n",
        "print(\"\\nEnter sentences to predict their sentiment:\")\n",
        "while True:\n",
        "    sentence = input(\"Enter a sentence (or type 'exit' to quit): \")\n",
        "    if sentence.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    sentence_vecs = {\n",
        "        \"TF-IDF\": tfidf_vectorizer.transform([sentence]),\n",
        "        \"Bag of Words\": bow_vectorizer.transform([sentence]),\n",
        "        \"Word2Vec\": get_w2v_features([sentence.split()], w2v_model, vector_size=100)\n",
        "    }\n",
        "\n",
        "    print(\"Predictions:\")\n",
        "    for method, vec in sentence_vecs.items():\n",
        "        pred_label = models[method].predict(vec)\n",
        "        sentiment = label_encoder.inverse_transform(pred_label)\n",
        "        print(f\"{method}: {sentiment[0]}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
